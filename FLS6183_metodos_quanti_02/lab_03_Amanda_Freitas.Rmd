```{r message=FALSE, warning=FALSE, include=FALSE}
# install.packages("corrplot")
# install.packages("lmtest")
# install.packages("olsrr")
# install.packages("stargazer")

library(lmtest)
library(olsrr)
library(corrplot)
library(tidyverse)
library(stargazer)
library(broom)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
dados <- read.csv("base_lab_03.csv")
head(dados)
```

4)	Rode quatro modelos introduzindo uma variável dependente por vez – primeiro x1, depois x2 - e depois as dependentes juntas x1 e x2, depois x1, x2 e c1 apenas. O que acontece com os coeficientes destas variáveis entre os modelos? Discuta;

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
modelo01 <- 
  dados %>% 
  lm(y1 ~ x1, data = .)

modelo02 <- 
  dados %>% 
  lm(y1 ~ x2, data = .)

modelo03 <- 
  dados %>% 
  lm(y1 ~ x1 + x2, data = .)

modelo04 <- 
  dados %>% 
  lm(y1 ~ x1 + x2 + c1, data = .)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
tidy(modelo01)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
tidy(modelo02)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
tidy(modelo03)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
tidy(modelo04)
```


```{r, results='asis'}
stargazer(modelo01, modelo02, modelo03, modelo04, 
          type = "html",
          digits =  2)
```

Com a introdução das variáveis X2. c1 e c2. a variável x1 diminui em significância estatística, mas a x2 mantém coeficiente próximo e mantém significancia estatística.

- o coeficiente pode ser significativo mas ter um valor muito pequeno, pois temos que considerar a unidade em que está o coeficiente.
- o esperado, ao introduzir novas variáveis explicativas e novos controles, é que mantenham a significância.
- quando se introduz variáveis e nenhuma explica, podem ser outros problemas. 
- Um R2 alto e nenhuma variável explicativa = problema.
- Um resultado desse tipo aponta para multicolineariedade. 
- Uma das formas de se detectar multicolineariedade é essa: introduzir variáveis uma por vez e vendo o que acontece na introdução de cada uma. 

## Multicolineariedade:
O problema de multicolineariedade surge no processo de inversão da matriz (ver fórmula de beta). Quando há colineariedade perfeita, a matriz não é inversível. Se uma variável tem alguma colineariedade, ou seja, está perfeitamente ou algo relacionado com outra, existe associação entre as variáveis e haverá problemas no seu modelo. Quando há uma variável vinculada à combinação das outras, a matriz inversa existirá, o beta será estimável, mas dificultará que haja significância estatística no seu modelo. 

Uma das formas de identificar isso é ver matriz de correlação entre as variáveis: se há variáveis com correlação muito próximas de 1, muito provavelmente haverá problema de multicolineariedade. Porém, se o problema de multicolineariedade for entre uma variável e a combinação entre outras, a matriz de correlação não será o suficiente para mostrar.

Preocupação maior com a explicação substantiva do que significam os coeficientes, e menos importância para p-valor e multicolineariedade. A multicolineariedade não viesa a estimativa do ponto, a multicolineariedade não interfere na estimativa do beta, a multicolinearidade influencia num erro padrão mais "inflado" do que deveria estar. 

A multicolineariedade está interferindo na soma dos resíduos (R2) e o erro padrão, atrapalhando também intervalo de confiança e p-valor. Mas não interferindo nas médias, isto é, nos betas. 

A matriz de correlação não mostrará, por exemplo, se x1 é multicolinear com x2 e x3 juntos, só vai mostrar sua correlação com x2 OU com x3. Então o problema é descobrir qual variável está produzindo a multicolineariedade. 


Variance Inflation Factor (VIF) = forma de encontrar multicolineariedade. 
Se o R2 for perto de 0, o VIF será perto de 1. Se o R2 está perto de 1, o VIF tenderá ao infinito. 

Também podemos descobrir multicolineariedade rodando modelos em que as variáveis X em função das outras variáveis e controles. Se houver baixa significância, o modelo consegue mostrar se há multicolinearieade medindo uma variável em presença das outras. 





5)	Agora rode um quinto modelo completo com as duas variáveis explicativas e as duas variáveis de controle. O que acontece com os coeficientes agora? Você detecta alguma alteração importante? Quais seriam os motivos?

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
modelo05 <- 
  dados %>% 
  lm(y1 ~ x1 + x2 + c1 + c2, data = .)
```

```{r}
modelo05 %>% 
  summary()
```

```{r}
confint(modelo05)
```

```{r, results='asis'}
stargazer(modelo01, modelo02, modelo03, modelo04, modelo05,
          type = "html",
          digits = 3)
```

A variável x1 agora não tem mais significância estatística e diminuiu muito em significância, mudou de sinal, e x2 perdeu um pouco de significância. Porém, olhando o intervalo de confiança, não dá para dizer que o coeficiente de x1 no modelo 03 e modelo 04 são diferentes, nem o x2.

Com o modelo 05, x1 troca de sinal e perde a capacidade de explicar, e o intervalo de confiança aumenta muito pois o erro-padrão vai de 4.3 para 14.3. 

Porém c2 na presença de x1 também não explica, "não roubou efeito" de x1. 






6)	Procure na lista de comandos do R para a aula de hoje um que permita checar se a suposta causa do comportamento observado considerado no item anterior.


```{r include=FALSE}
residuos_modelo_01 <- modelo01$residuals
residuos_modelo_02 <- modelo02$residuals
residuos_modelo_03 <- modelo03$residuals
residuos_modelo_04 <- modelo04$residuals
residuos_modelo_05 <- modelo05$residuals
```

Com essa informação, podemos contruir um gráfico para analisar nossos resíduos em comparação aos valores previstos pelo modelo usando o ggplot.

```{r message=FALSE, warning=FALSE, include=FALSE}
preditos_modelo_01 <- predict(modelo01)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
residuos_fitted_modelo01 <- 
  cbind(residuos_modelo_01, preditos_modelo_01) %>%
  as.data.frame()

residuos_fitted_modelo_01_residuos <- 
  as.data.frame(residuos_modelo_01)
```

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
ggplot(residuos_fitted_modelo01, aes(x = preditos_modelo_01, y = residuos_modelo_01)) +
  geom_point() + 
  stat_smooth(method='lm')
```



Há um outro teste necessário a ser feito com o intuito de se certificar de que não há violação às hipóteses básicas do modelo MQO. 

7)	Rode um modelo multivariado Y = f (X2, C1, C2), primeiro apenas com a única variável explicativa x2 e depois, incluindo os controles. Escreva a nova equação estimada;

```{r message=FALSE, warning=FALSE, include=FALSE}
modelo_07 <- 
  dados %>% 
  lm(y1 ~ x2, data = .)
  
modelo_06 <- 
dados %>% 
lm(y1 ~ x2 + c1 + c2, data = .)
```

```{r}
summary(modelo_06)
```

```{r}
plot(modelo04)
```


8)	Construa um gráfico de dispersão que relacione a variável explicativa com os resíduos. O que é possível notar neste gráfico a respeito da relação entre ambas as variáveis?


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
#Também podemos tirar um gráfico de dispersão entre resíduos e os valores previstos pelo modelo apenas com a função plot(). (Considere apenas o gráfico Residuals vs Fitted)
plot(modelo_06)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(modelo_07)
```

A média dos resíduos é 0, porém nesse gráfico à medida que a variável aumenta, a dispersão em torno da média 0 cresce cada vez mais.  Isso significa que a variância dos resíduos é uma função da variável c1, e isso viola uma das hipóteses do modelo de regressão e também dará problemas. Quando o gráfico forma um círculo significa que não há associação.


9)	Encontre um outro teste entre os comandos para a aula de hoje que permita testar se esta outra hipótese foi ou não violada. Reporte os resultados. Qual sua conclusão?

Podemos fazer teste de intervalo de confiança.


Para verificar a matriz de correlação entre as variáveis de uma tabela, usamos:
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
corrplot(cor(dados), 
         method = "number")
```

Na matriz de correlação entre as variáveis do banco, temos que x1 e c2 tem 0,9 de correlação, o que causará problema de multicolineariedade no modelo



A função bptest do pacote lmplot nos ajuda a verificar a heterocedasticidade do modelo:

```{r}
#bptest(reg)
#reg2 <- mtcars %>% lm(mpg ~., data=.)
#ols_vif_tol(reg2)
```


Nesta função, aplica-se um teste de hipótese no qual a hipótese nula corresponde aos resíduos estarem distribuídos com variância igual, e a hipótese alternativa aos resíduos estarem distribuídos com variância desigual.

Por fim, para testar a multicolinearidade entre os dados disponíveis, podemos usar a função ols_vif_tol do pacote olsrr.

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
ols_vif_tol(modelo05)
```

Com o VIF de 12,8 , temos que x1 e c2 são altamente correlacionadas.

O VIF quanto menor melhor, não há um valor de corte
  
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
ols_vif_tol(modelo04)
```
 No VIF do modelo 04, isto é, sem a variável c2, o VIF de x1 se torna 1,1.
 
 O VIF significa que num modelo de regressão com x1 como VD e c1 como VI, o R2 foi mais próximo de 1. No caso, o R2 deu 0,8:
 
```{r}
dados %>% 
lm(x1 ~ c2, data = .) %>% 
  summary()
```
 
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
ols_vif_tol(modelo05)
```

```{r}
ols_vif_tol(modelo_06)
```

VIF do modelo melhor

## Heterocedasticidade
Uma hipótese do modelo é a de que a variância é constante. O valor médio da estimativa da variância NÃO DEPENDE DE X. Testa se o resíduo é explicado por alguma das variáveis,o que é o mesmo que fazer uma regressão dos resíduos e das variáveis, esperamos que seja um modelo ruim.

```{r echo=TRUE, message=FALSE, warning=FALSE}
bptest(modelo_06)
```
Ho - a variância constante = homocedasticidade, isso quer dizer que esse modelo não é homocedástico. 

É possível corrigir o problema usando os erros-padrão ajustados. 
- Pegar vars que causam heterocedasiticdade "descontando" uma variância para remover essa heterocedasticidade.
- Só rodar modelo com erros padrão robustos se detectar a heterocedasticidade. 

